# ==============================================================================
#                 model_final_manual_loop_ultimate_fix.py (æœ€ç»ˆæ‰‹åŠ¨å¾ªç¯å®Œæ•´ç‰ˆ)
#
#   - [æ ¹æœ¬æ€§ä¿®æ­£] å½»åº•æ”¾å¼ƒæ‰€æœ‰ä¸å®˜æ–¹ Trainer ç›¸å…³çš„ä»£ç ã€‚
#   - å®Œå…¨å›å½’åˆ°å·²è¢«è¯æ˜å¯åœ¨ç”¨æˆ·ç¯å¢ƒä¸­å¯åŠ¨çš„ã€å®Œæ•´çš„æ‰‹åŠ¨PyTorchè®­ç»ƒå¾ªç¯ã€‚
#   - æ•´åˆäº†æ‰€æœ‰å·²ç¡®è®¤çš„å¿…è¦ä¿®æ­£ï¼š
#     1. æ­£ç¡®çš„ã€æ‰‹åŠ¨å±è”½æ ‡ç­¾çš„ Dataset å®ç°ã€‚
#     2. æ­£ç¡®çš„ã€è¿›è¡Œâ€œé—­å·è€ƒè¯•â€çš„ evaluate å‡½æ•°ã€‚
#     3. æ­£ç¡®çš„ã€å¤„ç†æ˜¾å­˜å’Œæ¨¡å‹åŠ è½½çš„ from_adapter å‡½æ•°ã€‚
#     4. æ­£ç¡®çš„ã€è¿›è¡Œå¤šæ¡ä»¶ä¿å­˜çš„ train å‡½æ•°é€»è¾‘ã€‚
#
# ==============================================================================
import json
import os
import gc  # <--- åŠ ä¸Šè¿™ä¸€è¡Œ
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader as TorchDataLoader
from sklearn.metrics import accuracy_score
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import get_peft_model, LoraConfig, PeftModel, prepare_model_for_kbit_training
import logging
from tqdm import tqdm
from torch.optim import AdamW
import bitsandbytes as bnb
import re
import random
import math
import sys

# --- è¾…åŠ©å‡½æ•° ---
def parse_llm_output_to_hard_label(llm_output_text: str, class_names: list) -> str:
    """
    é²æ£’çš„æ ‡ç­¾è§£æå‡½æ•°ã€‚
    ä¸ç®¡æ¨¡å‹è¾“å‡ºä»€ä¹ˆå¦–é­”é¬¼æ€ªï¼ˆæ¯”å¦‚é‡å¤ã€å¤§å°å†™ã€å¸¦æ ‡ç‚¹ï¼‰ï¼Œ
    éƒ½å¼ºåˆ¶æ˜ å°„å› class_names é‡Œçš„æ ‡å‡†åç§°ã€‚
    """
    if not isinstance(llm_output_text, str) or not class_names:
        return "Parse_Failed"
    
    # 1. é¢„å¤„ç†ï¼šè½¬å°å†™ï¼Œå»æ ‡ç‚¹
    text = llm_output_text.lower().strip()
    
    # 2. å®šä¹‰æ ‡å‡†ç±»åˆ«çš„å…³é”®è¯æ˜ å°„ (æ ¹æ®ä½ çš„æ•°æ®é›†è°ƒæ•´)
    # è¿™é‡Œçš„ Key æ˜¯ class_names é‡Œçš„æ ‡å‡†åï¼ŒValue æ˜¯å¯èƒ½çš„å˜ä½“
    # æ³¨æ„ï¼šPU/CWRU/XJTU çš„ class_names å¯èƒ½ä¸åŒï¼Œè¿™é‡Œåšé€šç”¨å¤„ç†
    
    # åŠ¨æ€æ„å»ºæ˜ å°„é€»è¾‘
    matched_label = None
    
    # ç­–ç•¥ï¼šåœ¨æ–‡æœ¬ä¸­æœç´¢æ ‡å‡†ç±»åˆ«å
    # æŒ‰ç…§é•¿åº¦å€’åºæœç´¢ï¼Œé˜²æ­¢ "Inner Race Fault" è¢« "Inner" æˆªèƒ¡
    sorted_classes = sorted(class_names, key=len, reverse=True)
    
    # ä¼˜å…ˆçœ‹ "Final Confirmed Diagnosis:" åé¢çš„å†…å®¹
    if "final confirmed diagnosis:" in text:
        target_area = text.split("final confirmed diagnosis:")[-1]
    else:
        target_area = text # æ²¡æ‰¾åˆ°æ ‡ç­¾å¤´ï¼Œå°±æœå…¨æ–‡
        
    for cls_name in sorted_classes:
        # å°†æ ‡å‡†åä¹Ÿè½¬å°å†™è¿›è¡ŒåŒ¹é…
        cls_lower = cls_name.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«
        if cls_lower in target_area:
            # æ‰¾åˆ°äº†ï¼ç›´æ¥è¿”å›æ ‡å‡†å (cls_name)
            # è¿™æ ·å³ä½¿ target_area æ˜¯ "normalnormal"ï¼Œåªè¦å®ƒåŒ…å« "normal"ï¼Œæˆ‘ä»¬è¿”å›çš„å°±æ˜¯ "Normal"
            matched_label = cls_name
            break
            
    if matched_label:
        return matched_label
        
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›å¤±è´¥
    return "Parse_Failed"

def find_all_linear_names(model):
    """Dynamically finds all linear layers for LoRA injection."""
    cls = (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear)
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[-1])
    if 'lm_head' in lora_module_names:
        lora_module_names.remove('lm_head')
    return sorted(list(lora_module_names))
class JudgeDataset(Dataset):
    """
    ä¸“é—¨ç”¨äºJudgeæ¨¡å‹è®­ç»ƒçš„Datasetç±»ã€‚
    å¤„ç†åŒ…å«ä¸¤ä¸ªæ¨¡å‹è¾“å‡ºå’Œè£å†³ç»“æœçš„è®­ç»ƒæ•°æ®ã€‚
    """
    def __init__(self, df, text_col, target_col, tokenizer, max_length, label_col_numeric):
        self.df = df
        self.texts = df[text_col].tolist()
        self.targets = df[target_col].tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å±æ€§éƒ½å­˜åœ¨
        # æ ‡ç­¾åˆ—
        if label_col_numeric in df.columns:
            self.true_labels = df[label_col_numeric].tolist()
        else:
            self.true_labels = [0] * len(df)
            logging.warning(f"Label column '{label_col_numeric}' not found, using default labels")
        
        self.label_col = label_col_numeric
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿true_winnerså±æ€§å­˜åœ¨
        if 'true_winner_str' in df.columns:
            self.true_winners = df['true_winner_str'].tolist()
        elif 'true_winner' in df.columns:
            self.true_winners = df['true_winner'].tolist()
        else:
            self.true_winners = [''] * len(df)
            logging.warning("'true_winner_str' and 'true_winner' columns not found, using empty strings")
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿true_fault_typeså±æ€§å­˜åœ¨
        if 'true_fault_type' in df.columns:
            self.true_fault_types = df['true_fault_type'].tolist()
        else:
            self.true_fault_types = [''] * len(df)
            logging.warning("'true_fault_type' column not found, using empty strings")
        
        logging.info(f"JudgeDataset initialized with {len(self.df)} samples")
        logging.info(f"  - true_winners: {len(self.true_winners)} entries")
        logging.info(f"  - true_fault_types: {len(self.true_fault_types)} entries")
        logging.info(f"  - true_labels: {len(self.true_labels)} entries")

    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        # è·å–instructionå’Œinput
        instruction = str(self.texts[idx])
        try:
            input_json = json.loads(self.df.iloc[idx]['input'])
        except:
            input_json = {} # å®¹é”™
        
        # ä»inputä¸­æå–å„ä¸ªéƒ¨åˆ†
        signal_data = input_json.get('signal_data', '')
        model_a_response = input_json.get('model_a_response', '')
        model_b_response = input_json.get('model_b_response', '')
        ground_truth = input_json.get('ground_truth', '')  # ç”¨äºè®­ç»ƒæ—¶çš„ç›‘ç£
        
        raw_output_text = str(self.targets[idx])
        
        # [ä¼˜åŒ–] æ¸…æ´— Outputï¼šå¦‚æœå¼€å¤´æœ‰ "json"ï¼ŒæŠŠå®ƒå»æ‰ï¼Œåªä¿ç•™ { ... }
        # è¿™æ ·æ¨¡å‹åœ¨æ¨ç†æ—¶ä¼šç›´æ¥è¾“å‡º JSONï¼Œè€Œä¸ä¼šè¾“å‡º "json" è¿™ä¸ªè¯
        actual_judge_json = self._clean_output_text(raw_output_text)
        
        # æ„å»º Prompt
        context = self._build_fault_diagnosis_prompt(instruction, signal_data, model_a_response, model_b_response)
        response = actual_judge_json + self.tokenizer.eos_token
        
        # Tokenizationè¿‡ç¨‹ä¿æŒä¸å˜
        context_encoding = self.tokenizer(context, add_special_tokens=True)
        response_encoding = self.tokenizer(response, add_special_tokens=False)

        context_ids = context_encoding['input_ids']
        response_ids = response_encoding['input_ids']

        # åˆå¹¶è¾“å…¥å’Œå“åº”
        input_ids = context_ids + response_ids
        
        # åˆ›å»ºæ ‡ç­¾ï¼šä¸Šä¸‹æ–‡éƒ¨åˆ†ä¸º-100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰ï¼Œå“åº”éƒ¨åˆ†ä¸ºå®é™…token
        labels = [-100] * len(context_ids) + response_ids

        # æˆªæ–­å¤„ç†
        if len(input_ids) > self.max_length:
            input_ids = input_ids[-self.max_length:]
            labels = labels[-self.max_length:]
        
        # å¡«å……å¤„ç†
        padding_length = self.max_length - len(input_ids)
        if padding_length > 0:
            # å·¦ä¾§å¡«å……
            input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids
            labels = [-100] * padding_length + labels
            attention_mask = [0] * padding_length + [1] * (self.max_length - padding_length)
        else:
            attention_mask = [1] * self.max_length

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'true_label': torch.tensor(self.true_labels[idx], dtype=torch.long),
            'context_len': torch.tensor(len(context_ids), dtype=torch.long),
            'original_indices': idx,
            'true_winner': self.true_winners[idx],
            'true_fault_type': self.true_fault_types[idx]  # ä¿®æ”¹ï¼šä½¿ç”¨æ•…éšœç±»å‹
        }
    def _clean_output_text(self, text):
        """
        æ¸…æ´— Teacher æ¨¡å‹ç”Ÿæˆçš„ Outputã€‚
        è¾“å…¥å¯èƒ½æ˜¯: "json\n{\n...}"
        æˆ‘ä»¬å¸Œæœ›è®­ç»ƒç›®æ ‡æ˜¯: "{\n...}" (çº¯ JSON)
        """
        text = text.strip()
        # å»æ‰ markdown ä»£ç å—æ ‡è®°
        if text.startswith("```json"):
            text = text[7:]
        if text.endswith("```"):
            text = text[:-3]
        
        # å»æ‰å¼€å¤´çš„ "json" å•è¯ (ä½ çš„æ•°æ®é‡Œæœ‰è¿™ä¸ª)
        if text.lower().startswith("json"):
            text = text[4:].strip()
            
        return text.strip()
    def _extract_json_from_response(self, judge_response):
        """ä»judge_responseä¸­æå–JSONå†…å®¹ï¼ˆå»æ‰```jsonå’Œ```æ ‡è®°ï¼‰"""
        try:
            if "```json" in judge_response:
                json_match = re.search(r'```json\s*(.*?)\s*```', judge_response, re.DOTALL)
                if json_match:
                    return json_match.group(1).strip()
            return judge_response.strip()
        except Exception:
            return judge_response
    def _build_fault_diagnosis_prompt(self, instruction, signal_data, model_a_response, model_b_response):
        prompt = f"""As a final arbiter and expert diagnostician, your task is to evaluate the reasoning processes of two different AI models (Model A and Model B) that have analyzed the same bearing signal data.

**Your Goal:** Determine which model provides a better, more logical, and more accurate diagnosis, and explain your reasoning in a detailed Chain-of-Thought.

**Evaluation Criteria:**
1.  **Logical Soundness:** Does the model's Chain-of-Thought logically follow from the provided data?
2.  **Accuracy:** Does the model's final conclusion match the Ground Truth?
3.  **Insightfulness:** Does the model correctly identify the key features that lead to the diagnosis?

**Important Constraints:**
- Keep your response concise and focused (100-200 words maximum).
- Provide your judgment in the specified JSON format only.
- Do not include any additional explanations outside the JSON structure.

---
**Original Signal Data (The "Exam Question"):**
{signal_data}

---
**Model A's Answer (Based on Time-Domain Features):**
{model_a_response}

---
**Model B's Answer (Based on Other Features):**
{model_b_response}

---
**Your Task:**
Provide your judgment in the following JSON format. Do not add any text before or after the JSON block.
json
{{
"thought": "First, I will analyze Model A's reasoning. Then I will analyze Model B's reasoning. I will compare both against the ground truth and the original signal data. Finally, I will decide which model performed better and state my final conclusion.",
"analysis_of_model_a": "...",
"analysis_of_model_b": "...",
"comparison_and_reasoning": "...",
"winner": "Model A"
}}
Note: The "winner" can be "Model A", "Model B", "Both are equally good", "Both are equally bad", or "Neither is correct"."""
        return prompt
    
class FlexibleLabelDataset(Dataset):
    """
    Dataset with the final, corrected variable definitions for SFT.
    """
    def __init__(self, df, text_col, target_col, tokenizer, max_length, label_col_numeric):
        self.df = df
        self.texts = df[text_col].tolist()
        self.targets = df[target_col].tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.true_labels = df[label_col_numeric].tolist()
        self.label_col = label_col_numeric

    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        source_text = str(self.texts[idx])
        target_text = str(self.targets[idx])

        context = f"""As a rotating machinery diagnostics expert, your task is to write a detailed Chain-of-Thought (CoT) process.
This CoT must logically explain how the data in the provided "Signal Analysis Summary" leads to its "Final Confirmed Diagnosis".

**Key requirements for your CoT:**
1.  Your reasoning must be clear, step-by-step, and based *only* on the provided data.
2.  Use professional and accurate terminology.
3.  **Keep your explanation concise, around 100 words.** This is a brief summary of your thoughts.
4.  **Give your reply in English.**
---
**Signal Analysis Summary:**
{source_text}
---

**Your Chain-of-Thought:**
"""
        response = target_text + self.tokenizer.eos_token
        
        # 1. åˆ†åˆ«Tokenize
        context_encoding = self.tokenizer(context, add_special_tokens=True)
        response_encoding = self.tokenizer(response, add_special_tokens=False)

        # 2. è·å–Token IDåˆ—è¡¨
        context_ids = context_encoding['input_ids']
        response_ids = response_encoding['input_ids']

        # 3. åˆå¹¶
        input_ids = context_ids + response_ids
        
        # 4. åˆ›å»ºæ ‡ç­¾
        labels = [-100] * len(context_ids) + response_ids
        
        # 5. æˆªæ–­å’Œå¡«å…… - ä¿®æ­£ä¸ºå·¦å¡«å……
        if len(input_ids) > self.max_length:
            # å¦‚æœè¶…é•¿ï¼Œä»å·¦ä¾§æˆªæ–­ï¼ˆä¿ç•™å³ä¾§é‡è¦å†…å®¹ï¼‰
            input_ids = input_ids[-self.max_length:]
            labels = labels[-self.max_length:]
        
        padding_length = self.max_length - len(input_ids)
        if padding_length > 0:
            # å·¦ä¾§å¡«å……
            input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids
            labels = [-100] * padding_length + labels
            attention_mask = [0] * padding_length + [1] * (self.max_length - padding_length)
        else:
            attention_mask = [1] * self.max_length

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'true_label': torch.tensor(self.true_labels[idx], dtype=torch.long),
            'context_len': torch.tensor(len(context_ids), dtype=torch.long),
            'original_indices': idx
        }


class BaseModel:
    def __init__(self, model_name, num_labels, lora_config_dict=None, tuning_method='qlora', load_adapter_from=None):
        self.model_name = model_name
        self.num_labels = num_labels
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logging.info(f"Initializing BaseModel with base model: {model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)
        self.tokenizer.padding_side = 'left'
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        if tuning_method == 'qlora':
            quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)
        elif tuning_method == 'lora':
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        else:
            quantization_config = None

        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_name, quantization_config=quantization_config, torch_dtype=torch.bfloat16,
                attn_implementation="flash_attention_2", device_map="auto", trust_remote_code=True)
        except Exception:
            model = AutoModelForCausalLM.from_pretrained(
                model_name, quantization_config=quantization_config, device_map="auto", trust_remote_code=True)
        
        model.resize_token_embeddings(len(self.tokenizer))
        model.config.pad_token_id = self.tokenizer.pad_token_id
        
        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
        
        if lora_config_dict:
            target_modules = find_all_linear_names(model)
            peft_config = LoraConfig(
                task_type="CAUSAL_LM", inference_mode=False, r=lora_config_dict.get('r', 16),
                lora_alpha=lora_config_dict.get('lora_alpha', 32), lora_dropout=lora_config_dict.get('lora_dropout', 0.1),
                target_modules=target_modules)
            if load_adapter_from and os.path.exists(load_adapter_from):
                self.model = PeftModel.from_pretrained(model, load_adapter_from)
            else:
                self.model = get_peft_model(model, peft_config)
            self.model.print_trainable_parameters()
        else:
            self.model = model
    
    
    def prepare_data_loader(self, df, text_col, target_col, batch_size, max_length, label_col_numeric, dataset_type='diagnosis', is_train=True):
        """
        å‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒä¸åŒç±»å‹çš„Dataset
        
        Args:
            dataset_type: 'diagnosis' ç”¨äºè¯Šæ–­æ¨¡å‹, 'judge' ç”¨äºJudgeæ¨¡å‹
        """
        if dataset_type == 'judge':
            dataset = JudgeDataset(df, text_col, target_col, self.tokenizer, max_length, label_col_numeric)
        else:
            dataset = FlexibleLabelDataset(df, text_col, target_col, self.tokenizer, max_length, label_col_numeric)
        
        if is_train:
            # è®­ç»ƒé›†ï¼šä¿æŒä¼ å…¥çš„ batch_size (é€šå¸¸æ˜¯ 1ï¼Œä¸ºäº†çœæ˜¾å­˜å­˜æ¢¯åº¦)
            final_batch_size = batch_size
        else:
            if dataset_type == 'judge':
                final_batch_size = 1  # <--- ä¿é™©èµ·è§ï¼Œè®¾ä¸º 1
                logging.info(f"Judgeæ¨¡å‹è¯„ä¼°ï¼šå¼ºåˆ¶ Batch Size = {final_batch_size} ä»¥é˜² OOMã€‚")
            else:
                final_batch_size = 4  # æ™®é€šæ¨¡å‹å¯ä»¥å¤§ä¸€ç‚¹  
            
            # å¦‚æœé‡åˆ° OOM (çˆ†æ˜¾å­˜)ï¼Œè¯·æŠŠè¿™é‡Œæ”¹æˆ 8 æˆ– 4
            logging.info(f"æ£€æµ‹åˆ°è¯„ä¼°æ¨¡å¼ï¼Œè‡ªåŠ¨å°† Batch Size ä» {batch_size} æå‡è‡³ {final_batch_size} ä»¥åŠ é€Ÿæ¨ç†ã€‚")
        return TorchDataLoader(dataset, batch_size=final_batch_size, shuffle=is_train, pin_memory=True, num_workers=4)

    def calculate_optimal_max_tokens(self, data_loader, sample_batches=20, buffer_ratio=1.2, max_cap=256):
        logging.info(f"Calculating optimal max_new_tokens from {sample_batches} batches...")
        target_lengths = []
        if not data_loader: return max_cap
        for i, batch in enumerate(data_loader):
            if i >= sample_batches: break
            labels = batch['labels']
            for sample_labels in labels:
                valid_length = (sample_labels != -100).sum().item()
                if valid_length > 0: target_lengths.append(valid_length)
        if not target_lengths: return max_cap
        p99_length = np.percentile(target_lengths, 99)
        optimal_length = math.ceil((p99_length * buffer_ratio) / 8) * 8
        final_length = min(optimal_length, max_cap)
        logging.info(f"Optimal max_new_tokens calculated: {final_length}")
        return final_length

    def train(self, train_loader, val_loader, epochs, learning_rate, output_dir, class_names=None, optimal_max_new_tokens=256, acc_freq=1):
        
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
        
        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
        best_val_acc = 0.0
        best_val_loss = float('inf')

        for epoch in range(epochs):
            self.model.train()
            self.model.config.use_cache = False # è®­ç»ƒæ—¶å…³é—­ Cache èŠ‚çœæ˜¾å­˜
            
            total_train_loss = 0
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} Training", leave=False, file=sys.stdout)
            
            for batch in progress_bar:
                # æ•°æ®æ¬è¿ (ç§»é™¤ä¸éœ€è¦æ¢¯åº¦çš„éTensoré¡¹)
                batch = {k: v.to(self.device) for k, v in batch.items() 
                         if k not in ['true_label', 'context_len']}
                
                optimizer.zero_grad()
                outputs = self.model(**batch)
                loss = outputs.loss
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_train_loss += loss.item()
                
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = total_train_loss / len(train_loader)
            history['train_loss'].append(avg_train_loss)
            
            # --- ä»¥ä¸‹æ˜¯éªŒè¯é€»è¾‘ (ä¿æŒä¹‹å‰çš„ä¼˜åŒ–) ---
            if val_loader is not None:
                # åˆ¤æ–­æ˜¯å¦è®¡ç®—å‡†ç¡®ç‡
                should_calc_acc = ((epoch + 1) % acc_freq == 0) or ((epoch + 1) == epochs)
                
                desc = f"Epoch {epoch+1} Val (Loss Only)"
                if should_calc_acc:
                    desc = f"Epoch {epoch+1} Val (Loss + Acc)"

                val_loss, val_acc = self.evaluate(
                    val_loader, class_names, 
                    description=desc,
                    max_new_tokens=optimal_max_new_tokens,
                    calc_acc=should_calc_acc 
                )
                
                history['val_loss'].append(val_loss)
                
                if should_calc_acc:
                    history['val_acc'].append(val_acc)
                    log_msg = f"Epoch {epoch+1}/{epochs} -> Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}"
                else:
                    history['val_acc'].append(None)
                    log_msg = f"Epoch {epoch+1}/{epochs} -> Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: (Skipped)"
                
                logging.info(log_msg)

                save_triggered = False
                reason = ""
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    reason += " New best loss!"
                    save_triggered = True
                
                if should_calc_acc and val_acc > best_val_acc:
                    best_val_acc = val_acc
                    reason += " New best accuracy!"
                    save_triggered = True
                
                if save_triggered:
                    best_model_path = os.path.join(output_dir, "best_model_lora")
                    self.model.save_pretrained(best_model_path)
                    self.tokenizer.save_pretrained(best_model_path)
                    logging.info(f"  ---> Model saved! Reason: {reason.strip()}.")
                
                scheduler.step(val_loss)
        
        return history
    
    # [ä¿®æ”¹] å¢åŠ  calc_acc å‚æ•°ï¼Œé»˜è®¤ True
    def evaluate(self, data_loader, class_names, description="Evaluating", max_new_tokens=256, calc_acc=True):
        self.model.eval()
        self.model.config.use_cache = True
        total_loss = 0
        all_preds = []
        all_ground_truth = []
        
        # åªæœ‰åœ¨éœ€è¦è®¡ç®—å‡†ç¡®ç‡æ—¶ï¼Œæ‰æ£€æŸ¥ pad_token
        if calc_acc:
            self.tokenizer.padding_side = 'left'
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        with torch.no_grad():
            for batch in tqdm(data_loader, desc=description, leave=False, file=sys.stdout):
                # 1. å‡†å¤‡æ•°æ® (è®¡ç®— Loss æ€»æ˜¯éœ€è¦çš„)
                tensor_batch = {k: v.to(self.device) for k, v in batch.items() 
                                if isinstance(v, torch.Tensor) and k not in ['context_len', 'true_label']}
                
                # 2. è®¡ç®— Validation Loss (æ¯ä¸€è½®éƒ½åšï¼Œé€Ÿåº¦å¿«)
                outputs = self.model(**tensor_batch)
                loss = outputs.loss
                total_loss += loss.item()

                # =================================================
                # [æ ¸å¿ƒæ§åˆ¶] åªæœ‰åœ¨ calc_acc=True æ—¶æ‰è¿›è¡Œç”Ÿæˆ
                # =================================================
                if calc_acc:
                    # æ”¶é›†çœŸå€¼
                    if 'true_label' in batch:
                        all_ground_truth.extend(batch['true_label'].numpy())

                    # --- ä»¥ä¸‹æ˜¯è€—æ—¶çš„ç”Ÿæˆé€»è¾‘ ---
                    input_ids_cpu = tensor_batch['input_ids'].cpu()
                    labels_cpu = tensor_batch['labels'].cpu()
                    masks_cpu = tensor_batch['attention_mask'].cpu()
                    
                    batch_prompt_input_ids = []
                    batch_prompt_attention_mask = []
                    
                    for i in range(input_ids_cpu.shape[0]):
                        answer_start_indices = (labels_cpu[i] != -100).nonzero()
                        if len(answer_start_indices) > 0:
                            cut_idx = answer_start_indices[0].item()
                            prompt_ids = input_ids_cpu[i, :cut_idx]
                            prompt_mask = masks_cpu[i, :cut_idx]
                        else:
                            prompt_ids = input_ids_cpu[i]
                            prompt_mask = masks_cpu[i]
                        
                        valid_start = (prompt_mask == 1).nonzero()
                        if len(valid_start) > 0:
                            real_start = valid_start[0].item()
                            prompt_ids = prompt_ids[real_start:]
                            prompt_mask = prompt_mask[real_start:]

                        batch_prompt_input_ids.append(prompt_ids)
                        batch_prompt_attention_mask.append(prompt_mask)

                    inputs_for_gen = self.tokenizer.pad(
                        {'input_ids': batch_prompt_input_ids, 'attention_mask': batch_prompt_attention_mask},
                        padding=True, return_tensors='pt'
                    ).to(self.device)

                    generated_ids = self.model.generate(
                        **inputs_for_gen,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        repetition_penalty=1.1 
                    )

                    input_len = inputs_for_gen['input_ids'].shape[1]
                    new_tokens = generated_ids[:, input_len:]
                    decoded_outputs = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)

                    for text in decoded_outputs:
                        pred = parse_llm_output_to_hard_label(text, class_names)
                        all_preds.append(pred)

        # å¾ªç¯ç»“æŸï¼Œè®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(data_loader)
        accuracy = 0.0
        
        if calc_acc:
            valid_indices = [i for i, p in enumerate(all_preds) if p != -1]
            min_len = min(len(all_ground_truth), len(all_preds))
            valid_gt = [all_ground_truth[i] for i in valid_indices if i < min_len]
            valid_pr = [all_preds[i] for i in valid_indices if i < min_len]
            if valid_gt:
                accuracy = accuracy_score(valid_gt, valid_pr)
                logging.info(f"è¯„ä¼°è¯¦ç»†: æœ‰æ•ˆæ ·æœ¬ {len(valid_gt)}/{min_len}, Acc: {accuracy:.4f}")
        
        self.model.config.use_cache = False
        return avg_loss, accuracy
    def predict(self, prompts: list, batch_size=16, max_new_tokens=256):
        """
        å¯¹åˆ—è¡¨ä¸­çš„ Prompts è¿›è¡Œé«˜æ•ˆçš„æ‰¹é‡é¢„æµ‹ã€‚
        """
        self.model.eval()
        self.model.config.use_cache = True
        
        # [å…³é”®] ç¡®ä¿ Tokenizer æ˜¯å·¦å¡«å…… (ç”Ÿæˆä»»åŠ¡å¿…å¤‡)
        self.tokenizer.padding_side = 'left'
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        all_outputs = []
        total = len(prompts)
        
        logging.info(f"å¼€å§‹æ‰¹é‡æ¨ç†ï¼Œæ€»æ ·æœ¬æ•°: {total}, Batch Size: {batch_size}")
        
        # æ‰¹é‡å¤„ç†å¾ªç¯
        for i in tqdm(range(0, total, batch_size), desc="Predicting", leave=False, file=sys.stdout):
            batch_prompts = prompts[i : i + batch_size]
            
            # Tokenize: è‡ªåŠ¨å¤„ç†å·¦å¡«å……å’Œ Attention Mask
            # max_length è®¾ç½®ä¸º 2048 é˜²æ­¢æä¸ªåˆ«è¶…é•¿ Prompt å¯¼è‡´ OOM
            inputs = self.tokenizer(
                batch_prompts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=2048
            ).to(self.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,        # è´ªå©ªè§£ç ï¼Œæœ€å¿«ä¸”ç¡®å®šæ€§é«˜
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # [å…³é”®] åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ† (è·³è¿‡ Input Prompt)
            input_len = inputs.input_ids.shape[1]
            new_tokens = generated_ids[:, input_len:]
            
            decoded_batch = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
            all_outputs.extend(decoded_batch)
            
        return all_outputs
    def _generate_sample_by_sample(self, batch, context_lengths, max_new_tokens, class_names, batch_size):
        """é€æ ·æœ¬ç”Ÿæˆçš„å¤‡ç”¨æ–¹æ³•ï¼ˆå¤„ç†å·¦å¡«å……ï¼‰"""
        batch_preds = []
        
        for i in range(batch_size):
            try:
                if context_lengths is not None and i < len(context_lengths):
                    context_len = min(context_lengths[i], batch['input_ids'].shape[1])
                    
                    # å¤„ç†å·¦å¡«å……ï¼šè·³è¿‡å·¦ä¾§å¡«å……ï¼Œå–å³ä¾§æœ‰æ•ˆå†…å®¹
                    valid_length = batch['attention_mask'][i].sum().item()
                    start_pos = batch['input_ids'].shape[1] - valid_length
                    end_pos = start_pos + min(context_len, valid_length)
                    
                    inputs_for_gen = {
                        "input_ids": batch['input_ids'][i:i+1, start_pos:end_pos],
                        "attention_mask": batch['attention_mask'][i:i+1, start_pos:end_pos]
                    }
                else:
                    # å›é€€æ–¹æ³•ï¼šä½¿ç”¨æ ‡ç­¾ä¼°ç®—ä¸Šä¸‹æ–‡é•¿åº¦
                    labels = batch['labels'][i]
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé-100çš„ä½ç½®ï¼ˆä¸Šä¸‹æ–‡ç»“æŸä½ç½®ï¼‰
                    non_pad_mask = (labels != -100)
                    if non_pad_mask.any():
                        context_end = non_pad_mask.nonzero(as_tuple=True)[0][0].item()
                        inputs_for_gen = {
                            "input_ids": batch['input_ids'][i:i+1, :context_end],
                            "attention_mask": batch['attention_mask'][i:i+1, :context_end]
                        }
                    else:
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œä½¿ç”¨æ•´ä¸ªåºåˆ—
                        inputs_for_gen = {
                            "input_ids": batch['input_ids'][i:i+1],
                            "attention_mask": batch['attention_mask'][i:i+1]
                        }
                
                generated_ids = self.model.generate(
                    **inputs_for_gen,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
                
                # è®¡ç®—è¾“å…¥é•¿åº¦
                input_len = inputs_for_gen['input_ids'].shape[1]
                
                decoded_output = self.tokenizer.decode(
                    generated_ids[0, input_len:], 
                    skip_special_tokens=True
                )
                pred_label_index = parse_llm_output_to_hard_label(decoded_output, class_names)
                batch_preds.append(pred_label_index)
                
            except Exception as e:
                logging.error(f"æ ·æœ¬ {i} ç”Ÿæˆå¤±è´¥: {e}")
                # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
                logging.debug(f"è¾“å…¥å½¢çŠ¶: {batch['input_ids'][i].shape if 'input_ids' in batch else 'N/A'}")
                logging.debug(f"ä¸Šä¸‹æ–‡é•¿åº¦: {context_lengths[i] if context_lengths is not None and i < len(context_lengths) else 'N/A'}")
                batch_preds.append(-1)  # æ ‡è®°ä¸ºæ— æ•ˆé¢„æµ‹
        
        return batch_preds
    def train_judge(self, train_loader, val_loader, val_df, epochs, learning_rate, output_dir, optimal_max_new_tokens=1024):
        """
        ä¿®æ­£ç‰ˆï¼š
        1. éªŒè¯å‰å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ã€‚
        2. æ¯ä¸ªEpochç»“æŸéƒ½ä¿å­˜ last_modelï¼Œé˜²æ­¢ç™½è·‘ã€‚
        3. å¢å¼ºOOMæ•è·æœºåˆ¶ï¼ŒéªŒè¯å¤±è´¥ä¸ä¸­æ–­è®­ç»ƒã€‚
        """
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        # ç§»é™¤ verbose=True ä»¥å…¼å®¹æ–°ç‰ˆ PyTorch
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
        
        history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_diagnosis_acc': []}
        best_val_acc = 0.0
        best_val_diagnosis_acc = 0.0
        best_val_loss = float('inf')

        for epoch in range(epochs):
            self.model.train()
            self.model.config.use_cache = False
            total_train_loss = 0
            
            # --- è®­ç»ƒå¾ªç¯ ---
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} Training", leave=False, file=sys.stdout)
            for batch in progress_bar:
                tensor_batch = {}
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor) and k not in ['context_len', 'true_label', 'true_winner', 'true_fault_type']:
                        tensor_batch[k] = v.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(**tensor_batch)
                loss = outputs.loss
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                total_train_loss += loss.item()
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            avg_train_loss = total_train_loss / len(train_loader)
            history['train_loss'].append(avg_train_loss)
            
            # ============================================================
            # [å…³é”®ä¿®æ­£ 1] å¼ºåˆ¶ä¿å­˜ Checkpoint (ä¿åº•ç­–ç•¥)
            # æ— è®ºéªŒè¯æ˜¯å¦æˆåŠŸï¼Œå…ˆä¿å­˜å½“å‰æ¨¡å‹ï¼Œé˜²æ­¢OOMå¯¼è‡´å‰åŠŸå°½å¼ƒ
            # ============================================================
            last_model_path = os.path.join(output_dir, "last_model_lora")
            self.model.save_pretrained(last_model_path)
            self.tokenizer.save_pretrained(last_model_path)
            logging.info(f"  --> [Checkpoint] Epoch {epoch+1} completed. Model saved to {last_model_path}")

            # ============================================================
            # [å…³é”®ä¿®æ­£ 2] éªŒè¯å‰æ¸…ç†æ˜¾å­˜
            # ============================================================
            torch.cuda.empty_cache()
            gc.collect()

            if val_loader is not None and val_df is not None:
                try:
                    # æ‰§è¡ŒéªŒè¯
                    val_loss, val_acc, val_diagnosis_acc, *rest= self.evaluate_judge(
                        val_loader, 
                        val_df, 
                        description=f"Epoch {epoch+1} Validation"
                    )
                    history['val_loss'].append(val_loss)
                    history['val_acc'].append(val_acc)
                    history['val_diagnosis_acc'].append(val_diagnosis_acc)
                    
                    logging.info(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, "
                            f"Val Acc: {val_acc:.4f}, Val Diagnosis Acc: {val_diagnosis_acc:.4f}")

                    # ä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘ (Best Model)
                    save_model = False
                    reason = ""
                    
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        save_model = True
                        reason += "New best accuracy!"
                    
                    if val_diagnosis_acc > best_val_diagnosis_acc:
                        best_val_diagnosis_acc = val_diagnosis_acc
                        save_model = True
                        reason += " New best diagnosis accuracy!"
                    
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        save_model = True
                        reason += " New best loss!"
                    
                    if save_model:
                        best_model_path = os.path.join(output_dir, "best_model_lora")
                        self.model.save_pretrained(best_model_path)
                        self.tokenizer.save_pretrained(best_model_path)
                        logging.info(f"  --> [Best Model] Saved! Reason: {reason.strip()}.")
                        
                    scheduler.step(val_loss)

                except RuntimeError as e:
                    # [å…³é”®ä¿®æ­£ 3] æ•è· OOM é”™è¯¯ï¼Œä¸è®©ç¨‹åºå´©æºƒ
                    if "out of memory" in str(e).lower():
                        logging.error(f"âš ï¸ è­¦å‘Š: éªŒè¯é˜¶æ®µå‘ç”Ÿæ˜¾å­˜æº¢å‡º (OOM)ï¼è·³è¿‡æœ¬è½®éªŒè¯ï¼Œç»§ç»­ä¸‹ä¸€è½®è®­ç»ƒã€‚")
                        logging.error("å»ºè®®: è¯·è¿›ä¸€æ­¥è°ƒå° `prepare_data_loader` ä¸­çš„éªŒè¯é›† Batch Sizeã€‚")
                        torch.cuda.empty_cache() # å†æ¬¡æ¸…ç†
                        history['val_loss'].append(float('inf'))
                    else:
                        logging.error(f"éªŒè¯è¯„ä¼°å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                except Exception as e:
                    logging.error(f"éªŒè¯è¯„ä¼°å¤±è´¥: {e}")
                    
        return history
    def evaluate_judge(self, data_loader, test_df, description="Evaluating Judge"):
        self.model.eval()
        self.model.config.use_cache = True
        total_loss = 0
        
        # è®°å½•åˆ—è¡¨
        all_pred_winners = []
        all_true_winners = []
        all_diagnosis_predictions = []
        all_ground_truths = []
        
        from collections import Counter
        bypass_count = 0
        # [æ–°å¢] è¯¦ç»†è®°å½•åˆ—è¡¨
        detailed_records = []
        # [ä¿®æ”¹ 1] å®šä¹‰æ›´ç²¾å‡†çš„å…³é”®è¯æ˜ å°„
        # åªè¦å‘½ä¸­åŒä¸€ä¸ª Key ä¸‹çš„ä»»æ„ Valueï¼Œå°±è§†ä¸ºè¯¥ç±»æ•…éšœ
        FAULT_MAP = {
            'Normal': ['normal', 'healthy'],
            'Inner': ['inner', 'irf'],
            'Outer': ['outer', 'orf'],
            'Ball': ['ball', 'rolling', 'element']
        }

        def get_fault_type(text):
            """æå–æ•…éšœç±»å‹çš„æ ¸å¿ƒç±»åˆ«"""
            if not isinstance(text, str): return "Unknown"
            t = text.lower()
            for key, keywords in FAULT_MAP.items():
                for kw in keywords:
                    if kw in t:
                        return key
            return "Unknown"

        def is_same_fault(pred1, pred2):
            """[å¢å¼ºç‰ˆ] åˆ¤æ–­ä¸¤ä¸ªè¯Šæ–­æ˜¯å¦ä¸€è‡´"""
            type1 = get_fault_type(pred1)
            type2 = get_fault_type(pred2)
            
            # å¦‚æœéƒ½è§£æå‡ºäº†æœ‰æ•ˆç±»å‹ï¼Œä¸”ç±»å‹ç›¸åŒ -> ä¸€è‡´
            if type1 != "Unknown" and type1 == type2:
                return True
            
            # å…œåº•ï¼šç®€å•çš„å­—ç¬¦ä¸²æ¯”å¯¹
            p1 = str(pred1).lower().strip()
            p2 = str(pred2).lower().strip()
            return p1 == p2

        def parse_judge_output(json_string):
            try:
                # [æ ¸å¿ƒä¿®å¤ 1] ç‰©ç†åˆ‡å‰²ï¼šåªä¿ç•™æœ€åä¸€ä¸ª '}' ä¹‹å‰çš„å†…å®¹
                # è¿™èƒ½å¹²æ‰æœ«å°¾çš„ "11111..." æˆ–é‡å¤çš„å†…å®¹
                if "}" in json_string:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªå…³é—­çš„å¤§æ‹¬å· (é’ˆå¯¹å•ä¸ªJSONçš„æƒ…å†µ)
                    # æˆ–è€…ï¼Œå¦‚æœå­˜åœ¨ ```json åŒ…è£¹ï¼Œä¼˜å…ˆæå–åŒ…è£¹å†…å®¹
                    if "```json" in json_string:
                        matches = re.findall(r'```json\s*(.*?)\s*```', json_string, re.DOTALL)
                        if matches: 
                            json_string = matches[0]
                    else:
                        # æ²¡æœ‰ markdownï¼Œå°è¯•æ‰¾æœ€å¤–å±‚çš„æ‹¬å·å¯¹
                        # ç®€å•ç­–ç•¥ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ª '{' å’Œå®ƒå¯¹åº”çš„é—­åˆ '}' æ¯”è¾ƒéš¾
                        # ç²—æš´ç­–ç•¥ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ª "winner" åŠå…¶åçš„ "}"
                        pass 
                
                # [æ ¸å¿ƒä¿®å¤ 2] å°è¯•è§£æ
                # ä½¿ç”¨ strict=False å…è®¸æ§åˆ¶å­—ç¬¦
                data = json.loads(json_string, strict=False)
                return data.get("winner", "Parse_Failed").strip()
                
            except json.JSONDecodeError:
                # [æ ¸å¿ƒä¿®å¤ 3] å¦‚æœæ ‡å‡†è§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™â€œå¾®åˆ›æ‰‹æœ¯â€æå– Winner
                # ä¸ç®¡ JSON ç»“æ„çƒ‚æˆä»€ä¹ˆæ ·ï¼Œåªè¦æœ‰ "winner": "Model A" å°±èƒ½æå‡ºæ¥
                match = re.search(r'"winner"\s*:\s*"(Model [AB]|Both.*?)"', json_string, re.IGNORECASE)
                if match:
                    return match.group(1).strip()
                    
                # å…³é”®è¯å…œåº• (æœ€åä¸€é“é˜²çº¿)
                if "Model A" in json_string: return "Model A"
                if "Model B" in json_string: return "Model B"
                if "Both" in json_string: return "Both are equally good"
                
                return "Parse_Failed"
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(data_loader, desc=description, leave=False, file=sys.stdout)):
                # --- 1. è·å–çœŸå®æ ‡ç­¾ ---
                try:
                    batch_true_winners = batch['true_winner']
                except KeyError:
                    if 'true_label' in batch:
                        label_to_winner = {0: 'Model A', 1: 'Model B', 2: 'Both are equally good'}
                        batch_true_winners = [label_to_winner.get(l.item(), 'Model A') for l in batch['true_label']]
                    else:
                        batch_true_winners = ['Unknown'] * len(batch['input_ids'])
                
                all_true_winners.extend(batch_true_winners)
                
                # --- 2. å‡†å¤‡æ•°æ® ---
                tensor_batch = {k: v.to(self.device) for k, v in batch.items() 
                                if isinstance(v, torch.Tensor) and k not in ['context_len', 'true_label', 'true_winner', 'true_fault_type']}
                
                outputs = self.model(**tensor_batch)
                total_loss += outputs.loss.item()
                
                # --- 3. æ¨ç†é€»è¾‘ ---
                context_lengths = (tensor_batch['labels'] == -100).sum(dim=1)
                
                for i in range(tensor_batch['input_ids'].shape[0]):
                    current_global_idx = batch_idx * data_loader.batch_size + i
                    
                    if current_global_idx < len(test_df):
                        row = test_df.iloc[current_global_idx]
                        sample_id = row.get('id', current_global_idx)
                        ground_truth = row.get('ground_truth', 'Unknown')
                        diag_a = row.get('model_a_diagnosis', 'Unknown')
                        diag_b = row.get('model_b_diagnosis', 'Unknown')
                    else:
                        sample_id = -1
                        ground_truth = "Unknown"
                        diag_a, diag_b = "Unknown", "Unknown"

                    all_ground_truths.append(ground_truth)
                    judge_raw_output = "SKIPPED (Smart Bypass)"
                    is_conflict = not is_same_fault(diag_a, diag_b)
                    if not is_conflict:
                        predicted_winner = "Both are equally good"
                        bypass_count += 1
                        final_diag = diag_a
                    else:
                        # [Call LLM]
                        context_len = context_lengths[i].item()
                        inputs_for_gen = {
                            "input_ids": tensor_batch['input_ids'][i:i+1, :context_len],
                            "attention_mask": tensor_batch['attention_mask'][i:i+1, :context_len]
                        }
                        
                        generated_ids = self.model.generate(
                            **inputs_for_gen,
                            max_new_tokens=1024,
                            do_sample=False,
                            pad_token_id=self.tokenizer.pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=1.1
                        )
                        
                        generated_text = self.tokenizer.decode(generated_ids[0, context_len:], skip_special_tokens=True)
                        judge_raw_output = generated_text # ä¿å­˜åŸå§‹è¾“å‡ºç”¨äºåˆ†æ thought
                        
                        predicted_winner = parse_judge_output(generated_text)
                        
                        if predicted_winner == "Model B":
                            final_diag = diag_b
                        else:
                            final_diag = diag_a

                    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    all_pred_winners.append(predicted_winner)
                    all_diagnosis_predictions.append(final_diag)
                    
                    # å°è¯•è·å– True Winner ç”¨äºè®¡ç®— Adjudication Acc
                    try:
                        t_winner_idx = batch['true_label'][i].item() if 'true_label' in batch else -1
                        # ç®€å•çš„ index è½¬ stringï¼Œä»…ç”¨äºè®°å½•
                        t_winner = "Unknown"
                        if t_winner_idx == 0: t_winner = "Model A"
                        elif t_winner_idx == 1: t_winner = "Model B"
                        elif t_winner_idx == 2: t_winner = "Both"
                        all_true_winners.append(t_winner)
                    except:
                        all_true_winners.append("Unknown")

                    # [æ ¸å¿ƒæ–°å¢] æ„å»ºè¯¦ç»†è®°å½•
                    # åªè®°å½•åˆ†æ­§æ ·æœ¬ï¼Œæˆ–è€…å…¨éƒ¨è®°å½•ï¼ˆå»ºè®®å…¨éƒ¨è®°å½•ï¼Œåé¢åˆ†ææ—¶å†è¿‡æ»¤ï¼‰
                    if current_global_idx < len(test_df):
                        
                        # [ä¿®å¤] å®‰å…¨åœ°è½¬æ¢ ID
                        try:
                            # å°è¯•è½¬ä¸º int
                            safe_id = int(sample_id)
                        except (ValueError, TypeError):
                            # å¦‚æœæ˜¯ NaN æˆ–æ— æ³•è½¬æ¢ï¼Œä½¿ç”¨å…¨å±€ç´¢å¼•ä»£æ›¿ï¼Œæˆ–è€…è®¾ä¸º -1
                            safe_id = int(current_global_idx)

                        record = {
                            "id": safe_id,  # <--- ä½¿ç”¨ä¿®å¤åçš„ safe_id
                            "is_conflict": bool(is_conflict),
                            "ground_truth": ground_truth,
                            "model_a_pred": diag_a,
                            "model_b_pred": diag_b,
                            "judge_winner": predicted_winner,
                            "final_diagnosis": final_diag,
                            "is_correct": bool(final_diag == ground_truth),
                            "judge_raw_output": judge_raw_output
                        }
                        detailed_records.append(record)

        # --- è®¡ç®—æŒ‡æ ‡ ---
        avg_loss = total_loss / len(data_loader)
        winner_distribution = dict(Counter(all_pred_winners))
        logging.info(f"\nğŸ“Š Judge é€‰æ‹©åˆ†å¸ƒ: {winner_distribution}")
        logging.info(f"âš¡ Smart Bypass (Both) è§¦å‘æ¬¡æ•°: {bypass_count}/{len(all_ground_truths)}")
        
        # 1. ä¸¥æ ¼è£å†³å‡†ç¡®ç‡ (Strict Accuracy)
        valid_pairs = [(t, p) for t, p in zip(all_true_winners, all_pred_winners) if t and p != "Parse_Failed"]
        if valid_pairs:
            vt, vp = zip(*valid_pairs)
            strict_acc = accuracy_score(vt, vp)
            logging.info(f"âš–ï¸ ä¸¥æ ¼è£å†³å‡†ç¡®ç‡ (Strict Acc): {strict_acc:.4f}")
            
            # [ä¿®æ”¹ 2] æ¾å¼›è£å†³å‡†ç¡®ç‡ (Relaxed Accuracy)
            # é€»è¾‘ï¼šå¦‚æœ Truth æ˜¯ Bothï¼Œé‚£ä¹ˆé€‰ A æˆ– B æˆ– Both éƒ½ç®—å¯¹
            relaxed_correct = 0
            for t, p in valid_pairs:
                if t == p:
                    relaxed_correct += 1
                elif t == "Both are equally good" and p in ["Model A", "Model B"]:
                    relaxed_correct += 1
            
            relaxed_acc = relaxed_correct / len(valid_pairs)
            logging.info(f"ğŸ¤ æ¾å¼›è£å†³å‡†ç¡®ç‡ (Relaxed Acc): {relaxed_acc:.4f} (å« Both å…¼å®¹)")
            
        else:
            strict_acc = 0.0
            logging.warning("æ— æœ‰æ•ˆè£å†³æ ·æœ¬")

        # 2. è¯Šæ–­å‡†ç¡®ç‡
        valid_diag = [(t, p) for t, p in zip(all_ground_truths, all_diagnosis_predictions) if t!="Unknown" and p!="Unknown"]
        valid_ground_truths = []
        valid_diagnosis_preds = []
        diag_acc = 0.0
        
        if valid_diag:
            valid_ground_truths, valid_diagnosis_preds = zip(*valid_diag)
            diag_acc = accuracy_score(valid_ground_truths, valid_diagnosis_preds)
            logging.info(f"ğŸ† è¯Šæ–­å‡†ç¡®ç‡ (Diagnosis Acc): {diag_acc:.4f}")
        
        self.model.config.use_cache = False
        
        # è¿”å›å€¼ä¿æŒä¸å˜ï¼Œç”¨ strict_acc å…¼å®¹æ—§æ¥å£
        return avg_loss, strict_acc, diag_acc, valid_ground_truths, valid_diagnosis_preds, winner_distribution, detailed_records
    def predict_judge(self, prompt: str, max_new_tokens: int = 1024) -> str:
        """
        ä¿®æ”¹ï¼šè¿”å›è£å†³ç»“æœå’Œå¯¹åº”çš„è¯Šæ–­é¢„æµ‹
        """
        self.model.eval()
        self.model.config.use_cache = True
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.2,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç ç”Ÿæˆéƒ¨åˆ†
        decoded_output = self.tokenizer.decode(generated_ids[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        # è§£æwinner
        def parse_winner_from_output(json_string):
            try:
                if "```json" in json_string:
                    json_match = re.search(r'```json\s*(.*?)\s*```', json_string, re.DOTALL)
                    if json_match:
                        json_string = json_match.group(1)
                
                data = json.loads(json_string)
                winner = data.get("winner", "").strip()
                return winner
            except:
                return "Parse_Failed"
        
        winner = parse_winner_from_output(decoded_output)
        
        return {
            'raw_output': decoded_output,
            'winner': winner,
            'diagnosis_insight': f"Judgeé€‰æ‹©äº†{winner}ï¼Œå¯¹åº”çš„è¯Šæ–­ç»“æœå°†åŸºäºè¯¥æ¨¡å‹çš„è¾“å‡º"
        }
    @classmethod
    def from_adapter(cls, base_model_name, adapter_dir, tuning_method='qlora'):
        logging.info(f"Loading model from adapter: {adapter_dir}")
        if tuning_method == 'qlora':
            quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)
        elif tuning_method == 'lora':
            quant_config = BitsAndBytesConfig(load_in_8bit=True)
        else:
            quant_config = None
        tokenizer = AutoTokenizer.from_pretrained(adapter_dir)
        tokenizer.padding_side = 'left'
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name, quantization_config=quant_config, torch_dtype=torch.bfloat16,
            device_map="auto", trust_remote_code=True, attn_implementation="eager")
        base_model.resize_token_embeddings(len(tokenizer))
        
        # Load adapter AFTER resizing
        model = PeftModel.from_pretrained(base_model, adapter_dir)
        
        logging.info("Merging LoRA adapter for inference...")
        model = model.merge_and_unload()
        logging.info("Adapter merged.")
        
        instance = object.__new__(cls)
        instance.model = model
        instance.tokenizer = tokenizer
        instance.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        return instance