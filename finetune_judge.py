import os
import argparse
import pandas as pd
from pathlib import Path
import logging
import numpy as np
import torch
import json
import sys
import time
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import re

# --- æ¨¡å—å¯¼å…¥ ---
try:
    from src.models.model import BaseModel
    # æˆ‘ä»¬ä¸å†éœ€è¦ parse_llm_output_to_hard_label
    from src.utils.visualization import plot_training_history
    print("æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ã€‚")
except ImportError as e:
    print(f"é”™è¯¯ï¼šæ— æ³•å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ã€‚Error: {e}"); sys.exit(1)

# --- å…¨å±€é…ç½®ï¼šä¸åŒæ•°æ®é›†çš„æ•…éšœç±»åˆ« ---
DATASET_CONFIGS = {
    'cwru': ['Normal', 'Ball Fault', 'Inner Race Fault', 'Outer Race Fault'],
    'pu':   ['Normal', 'Inner Race Fault', 'Outer Race Fault'], # PU é»˜è®¤ 3 ç±»
    'xjtu': ['Normal', 'Inner Race Fault', 'Outer Race Fault']
}

def get_correct_winner(model_a_pred: str, model_b_pred: str, ground_truth: str) -> str:
    a_is_correct = model_a_pred == ground_truth
    b_is_correct = model_b_pred == ground_truth

    if a_is_correct and b_is_correct:
        return "Both are equally good"
    elif a_is_correct and not b_is_correct:
        return "Model A"
    elif not a_is_correct and b_is_correct:
        return "Model B"
    else: 
        # [æ ¸å¿ƒä¿®æ”¹] å³ä½¿ä¸¤ä¸ªéƒ½é”™ï¼Œä¹Ÿä¸è¦è¿”å› "Neither"ã€‚
        # è¿”å› "Model A" ä½œä¸ºé»˜è®¤å…œåº• (å› ä¸ºAé€šå¸¸å‡†ç¡®ç‡ç•¥é«˜)
        # è¿™æ ·ä¿è¯äº†æ ‡ç­¾æ°¸è¿œåœ¨åˆæ³•èŒƒå›´å†…ã€‚
        return "Model A" 

def parse_judge_winner(json_string: str) -> str:
    """Parses the 'winner' field from the Judge's JSON output."""
    try:
        # 1. å°è¯•æ ‡å‡† JSON è§£æ
        if "```json" in json_string:
            json_match = re.search(r'```json\s*(.*?)\s*```', json_string, re.DOTALL)
            if json_match:
                json_string = json_match.group(1)
        
        data = json.loads(json_string)
        winner = data.get("winner", "Parse_Failed")
        return winner.strip()
    except:
        # 2. [æ–°å¢] æš´åŠ›æ­£åˆ™åŒ¹é… (å¦‚æœ JSON è§£æå¤±è´¥)
        # ç›´æ¥åœ¨æ–‡æœ¬é‡Œæ‰¾ "winner": "Model A" è¿™ç§æ¨¡å¼
        match = re.search(r'"winner"\s*:\s*"(.*?)"', json_string)
        if match:
            return match.group(1).strip()
            
        # 3. [æ–°å¢] å…³é”®è¯å…œåº•
        # å¦‚æœæ¨¡å‹ç›´æ¥è¾“å‡ºäº† "Model A" è€Œä¸æ˜¯ JSON
        if "Model A" in json_string and "Model B" not in json_string:
            return "Model A"
        if "Model B" in json_string and "Model A" not in json_string:
            return "Model B"
            
        return "Parse_Failed"

# --- å‘½ä»¤è¡Œå‚æ•°è§£æ (å·²ä¿®æ­£) ---
def parse_args():
    parser = argparse.ArgumentParser(description='Fine-tune a Large Language Model as a Judge.')
    
    # [æ–°å¢] æ•°æ®é›†é€‰æ‹©
    parser.add_argument('--dataset', type=str, default='xjtu', choices=['cwru', 'pu', 'xjtu'], 
                        help='Dataset name (cwru or pu).')
    
    # [ä¿®æ”¹] jsonl_path å˜ä¸ºå¯é€‰ï¼Œé»˜è®¤æ ¹æ® dataset ç”Ÿæˆ
    parser.add_argument('--jsonl_path', type=str, default=None, 
                        help='Override path to the .jsonl file. If None, auto-generated based on dataset.')
    
    parser.add_argument('--model_name', type=str, default='models/Qwen2.5-7B-Instruct', help='Base pretrained model for the Judge.')
    parser.add_argument('--output_dir', type=str, default='outputs/judge_model', help='Root directory for the fine-tuned model.')
    parser.add_argument('--tuning_method', type=str, default='qlora', choices=['lora', 'qlora'], help="Fine-tuning method.")
    parser.add_argument('--batch_size', type=int, default=1, help='Training batch size.')
    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs.')#puå¾—å¤šè·‘å‡ è½®
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate.')
    parser.add_argument('--max_length', type=int, default=3000, help='Maximum sequence length.')
    parser.add_argument('--lora_r', type=int, default=32, help='LoRA rank.')
    parser.add_argument('--lora_alpha', type=int, default=64, help='LoRA alpha.')
    parser.add_argument('--lora_dropout', type=float, default=0.1, help='LoRA dropout.')
     # [æ–°å¢] è®­ç»ƒæ—¶å‰”é™¤å…±è¯†æ ·æœ¬çš„å¼€å…³
    parser.add_argument('--exclude_consensus', action='store_true', 
                        help='If set, removes "Both are equally good" samples from the Training set ONLY.')
    return parser.parse_args()

def train_and_evaluate(args, train_df, val_df, test_df, winner_categories):
    # [ä¿®æ”¹] ç­–ç•¥åç§°åŠ å…¥ dataset å‰ç¼€
    strategy_name = f"{args.dataset}_{args.tuning_method}_Judge"
    
    logging.info(f"\n==================== Starting Judge SFT: {strategy_name.upper()} ====================")
    output_dir = Path(args.output_dir) / strategy_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. åˆå§‹åŒ–BaseModel
    try:
        model_instance = BaseModel(
            model_name=args.model_name, 
            num_labels=len(winner_categories),
            lora_config_dict={'r': args.lora_r, 'lora_alpha': args.lora_alpha, 'lora_dropout': args.lora_dropout}, 
            tuning_method=args.tuning_method)
    except Exception as e:
        logging.error(f"Failed to initialize BaseModel: {e}")
        return

    # 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨
    text_col, target_col = 'instruction', 'output'
    true_label_col = 'true_winner_label'
    
    train_loader = model_instance.prepare_data_loader(
        train_df, text_col, target_col, args.batch_size, args.max_length, 
        true_label_col, dataset_type='judge', is_train=True
    )
    val_loader = model_instance.prepare_data_loader(
        val_df, text_col, target_col, args.batch_size, args.max_length, 
        true_label_col, dataset_type='judge', is_train=False
    )
    
    # 3. è®­ç»ƒæ¨¡å‹
    logging.info("Starting training with diagnosis metrics...")
    
    optimal_max_new_tokens = model_instance.calculate_optimal_max_tokens(train_loader, max_cap=1024)
    
    try:
        learning_rate = float(args.learning_rate)
    except (ValueError, TypeError):
        learning_rate = 2e-5
    
    history = model_instance.train_judge(
        train_loader=train_loader,
        val_loader=val_loader, 
        val_df=val_df,
        epochs=args.epochs, 
        learning_rate=learning_rate,
        output_dir=output_dir, 
        optimal_max_new_tokens=optimal_max_new_tokens
    )
    
    # 4. æœ€ç»ˆè¯„ä¼°
    best_adapter_path = output_dir / 'best_model_lora'
    if not best_adapter_path.exists():
        logging.error("Best model not found. Evaluation skipped.")
        return

    logging.info("\n=== Final Evaluation with Diagnosis Metrics ===")
    
    del model_instance
    torch.cuda.empty_cache()
    gc.collect()
    time.sleep(2)

    try:
        eval_model_instance = BaseModel.from_adapter(
            base_model_name=args.model_name, 
            adapter_dir=str(best_adapter_path),
            tuning_method=args.tuning_method
        )
    except Exception as e:
        logging.error(f"Failed to load adapter: {e}")
        return

    test_loader = eval_model_instance.prepare_data_loader(
        test_df, text_col, target_col, args.batch_size, args.max_length, 
        true_label_col, dataset_type='judge', is_train=False
    )
    
    # [ä¿®æ”¹] æ¥æ”¶ 5 ä¸ªè¿”å›å€¼
    test_loss, test_adj_acc, test_diag_acc, true_labels, pred_labels, winner_dist, detailed_records = eval_model_instance.evaluate_judge(
        test_loader, test_df, description="Final Test"
    )
    
    # [æ–°å¢] ä¿å­˜åˆ†å¸ƒç»Ÿè®¡åˆ°æ–‡ä»¶
    dist_file = output_dir / "judge_choices_distribution.json"
    with open(dist_file, "w", encoding="utf-8") as f:
        json.dump(winner_dist, f, indent=2, ensure_ascii=False)
        
    logging.info(f"ğŸ“Š Judge é€‰æ‹©åˆ†å¸ƒå·²ä¿å­˜è‡³: {dist_file}")
    logging.info(f"å†…å®¹é¢„è§ˆ: {winner_dist}")
    
    # [æ–°å¢] ç”Ÿæˆå¹¶ä¿å­˜ Classification Report
    if true_labels and pred_labels:
        logging.info("Generating classification report...")
        report = classification_report(true_labels, pred_labels, digits=4, zero_division=0)
        
        report_path = output_dir / "classification_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)
            
        logging.info(f"ğŸ“ Classification report saved to: {report_path}")
        logging.info(f"\n{report}") # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    else:
        logging.warning("âš ï¸ No valid predictions to generate report.")
    
    # ä¿å­˜ç»“æœ
    test_results = {
        'dataset': args.dataset,
        'test_loss': float(test_loss),
        'test_adjudication_accuracy': float(test_adj_acc),
        'test_diagnosis_accuracy': float(test_diag_acc),
        'test_samples': len(test_df),
        'winner_categories': winner_categories,
        'winner_distribution': winner_dist # æŠŠåˆ†å¸ƒä¹Ÿå†™è¿›æ€»ç»“æœ
    }
    conflict_analysis_file = output_dir / f"conflict_analysis_{args.dataset}.jsonl"
    logging.info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†æ­§æ ·æœ¬è¯¦ç»†åˆ†æè‡³: {conflict_analysis_file}")
    
    conflict_count = 0
    correct_rescue_count = 0
    
    with open(conflict_analysis_file, "w", encoding="utf-8") as f:
        for record in detailed_records:
            # å†™å…¥æ–‡ä»¶
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
            # é¡ºä¾¿ç»Ÿè®¡ä¸€ä¸‹
            if record['is_conflict']:
                conflict_count += 1
                if record['is_correct']:
                    correct_rescue_count += 1
    
    logging.info(f"âš”ï¸ æ€»åˆ†æ­§æ ·æœ¬æ•°: {conflict_count}")
    if conflict_count > 0:
        logging.info(f"âœ… Judge æˆåŠŸæ•‘å› (Correct Rescue): {correct_rescue_count} ({correct_rescue_count/conflict_count:.2%})")
    else:
        logging.info("âš ï¸ æœ¬æ¬¡æµ‹è¯•æœªå‘ç°åˆ†æ­§æ ·æœ¬ã€‚")

    results_path = os.path.join(output_dir, "test_results_with_diagnosis.json")
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    # 5. å®šæ€§è¯„ä¼° - æ˜¾ç¤ºè£å†³å’Œè¯Šæ–­ç»“æœ
    logging.info("\n=== Qualitative Evaluation (Adjudication + Diagnosis) ===")
    sample_indices = [0, 1, 2]  # æµ‹è¯•é›†çš„å‰å‡ ä¸ªæ ·æœ¬
    
    for i, sample_idx in enumerate(sample_indices):
        if sample_idx >= len(test_df):
            break
            
        sample_row = test_df.iloc[sample_idx]
        input_data = json.loads(sample_row['input'])
        
        # é‡å»ºPrompt
        signal_data = input_data.get('signal_data', '')
        model_a_response = input_data.get('model_a_response', '')
        model_b_response = input_data.get('model_b_response', '')
        ground_truth = input_data.get('ground_truth', '')
        prompt = f"""As a final arbiter and expert diagnostician, your task is to evaluate the reasoning processes of two different AI models (Model A and Model B) that have analyzed the same bearing signal data.

**Your Goal:** Determine which model provides a better, more logical, and more accurate diagnosis, and explain your reasoning in a detailed Chain-of-Thought.

**Evaluation Criteria:**
1.  **Logical Soundness:** Does the model's Chain-of-Thought logically follow from the provided data?
2.  **Accuracy:** Does the model's final conclusion match the Ground Truth?
3.  **Insightfulness:** Does the model correctly identify the key features that lead to the diagnosis?

**Important Constraints:**
- Keep your response concise and focused (100-200 words maximum).
- Provide your judgment in the specified JSON format only.
- Do not include any additional explanations outside the JSON structure.

---
**Original Signal Data (The "Exam Question"):**
{signal_data}

---
**Model A's Answer (Based on Time-Domain Features):**
{model_a_response}

---
**Model B's Answer (Based on Other Features):**
{model_b_response}

---
**Your Task:**
Provide your judgment in the following JSON format. Do not add any text before or after the JSON block.
json
{{
"thought": "First, I will analyze Model A's reasoning. Then I will analyze Model B's reasoning. I will compare both against the ground truth and the original signal data. Finally, I will decide which model performed better and state my final conclusion.",
"analysis_of_model_a": "...",
"analysis_of_model_b": "...",
"comparison_and_reasoning": "...",
"winner": "..."
}}
Note: The "winner" can be "Model A", "Model B", "Both are equally good"."""
        
        logging.info(f"\n--- Sample {i+1} ---")
        logging.info("Signal Data (abbreviated):")
        logging.info(signal_data[:200] + "..." if len(signal_data) > 200 else signal_data)
        
        logging.info(f"\nGround Truth: {ground_truth}")
        logging.info(f"Model A Diagnosis: {sample_row.get('model_a_diagnosis', 'Unknown')}")
        logging.info(f"Model B Diagnosis: {sample_row.get('model_b_diagnosis', 'Unknown')}")
        
        # ä½¿ç”¨é¢„æµ‹æ–¹æ³•
        result = eval_model_instance.predict_judge(prompt, max_new_tokens=1024)
        
        logging.info(f"\nJudge Prediction:")
        logging.info(f"Raw Output: {result['raw_output']}")
        logging.info(f"Predicted Winner: {result['winner']}")
        logging.info(f"Actual Winner: {sample_row['true_winner']}")
        
        # è®¡ç®—è£å†³æ˜¯å¦æ­£ç¡®
        adjudication_correct = (result['winner'] == sample_row['true_winner'])
        logging.info(f"Adjudication Correct: {adjudication_correct}")
        
        # è®¡ç®—è¯Šæ–­æ˜¯å¦æ­£ç¡®
        if result['winner'] == "Model A":
            predicted_diagnosis = sample_row.get('model_a_diagnosis', 'Unknown')
        elif result['winner'] == "Model B":
            predicted_diagnosis = sample_row.get('model_b_diagnosis', 'Unknown')
        elif result['winner'] in ["Both are equally good", "Both are equally bad"]:
            # å¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½å¥½æˆ–éƒ½å·®ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹çš„è¯Šæ–­
            predicted_diagnosis = sample_row.get('model_a_diagnosis', 'Unknown')
        else:
            predicted_diagnosis = "Unknown"
        
        diagnosis_correct = (predicted_diagnosis == ground_truth)
        logging.info(f"Diagnosis Correct: {diagnosis_correct} (Predicted: {predicted_diagnosis}, Actual: {ground_truth})")
        
        if i >= 2:
            logging.info("... (more samples available)")
            break
    
    # 6. æ‰“å°æœ€ç»ˆç»“æœ
    logging.info(f"\n=== Final Results ===")
    logging.info(f"Adjudication Accuracy (è£å†³å‡†ç¡®ç‡): {test_adj_acc:.4f}")
    logging.info(f"Diagnosis Accuracy (è¯Šæ–­å‡†ç¡®ç‡): {test_diag_acc:.4f}")
    logging.info(f"Test Loss: {test_loss:.4f}")
    
    if history['val_acc']:
        best_epoch = np.argmax(history['val_acc'])
        logging.info(f"Best Validation Accuracy: {history['val_acc'][best_epoch]:.4f} (epoch {best_epoch + 1})")
    
    # 7. ä¿å­˜è®­ç»ƒå†å²
    training_history = {
        'train_loss': history['train_loss'],
        'val_loss': history['val_loss'],
        'val_acc': history['val_acc'],
        'best_epoch': int(np.argmax(history['val_acc'])) if history['val_acc'] else 0,
        'best_val_acc': float(max(history['val_acc'])) if history['val_acc'] else 0.0
    }
    
    history_path = os.path.join(output_dir, "training_history.json")
    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(training_history, f, indent=2, ensure_ascii=False)
    
    # 8. ä¿å­˜æ¨¡å‹é…ç½®
    config_info = {
        'model_name': args.model_name,
        'tuning_method': args.tuning_method,
        'lora_r': args.lora_r,
        'lora_alpha': args.lora_alpha,
        'epochs': args.epochs,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'max_length': args.max_length,
        'task_type': 'judge_with_diagnosis_metrics'
    }
    config_path = os.path.join(output_dir, "model_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_info, f, indent=2, ensure_ascii=False)
    
    logging.info(f"\n=== Judge Training with Diagnosis Metrics Complete! ===")
    logging.info(f"Outputs saved to: {output_dir}")
    
    return {
        'test_adjudication_accuracy': test_adj_acc,
        'test_diagnosis_accuracy': test_diag_acc,
        'test_loss': test_loss,
        'output_dir': str(output_dir)
    }
    

def main():
    args = parse_args()
    
    # [æ–°å¢] è‡ªåŠ¨æ„å»ºæ–‡ä»¶è·¯å¾„
    if args.jsonl_path is None:
        args.jsonl_path = f"finetuning_dataset_for_judge_{args.dataset}_balanced.jsonl"
    
    # 1. åŠ è½½Judgeæ•°æ®é›†
    jsonl_path = Path(args.jsonl_path)
    if not jsonl_path.exists():
        logging.error(f"Judge dataset not found: {jsonl_path.resolve()}")
        logging.error(f"è¯·ç¡®ä¿æ‚¨å·²ç»è¿è¡Œäº† local_generate_judge_dataset.py --dataset {args.dataset}")
        return
    
    logging.info(f"Loading Judge dataset from {jsonl_path}...")
    data_df = pd.read_json(jsonl_path, lines=True)
    logging.info(f"Original dataset size: {len(data_df)}")
    
    def is_valid_input_json(text):
        if not isinstance(text, str): return False
        try:
            json.loads(text)
            return True
        except:
            return False

    def is_valid_output(text):
        # output åªè¦æ˜¯å­—ç¬¦ä¸²ä¸”æœ‰å†…å®¹å³å¯ï¼Œä¸éœ€è¦æ˜¯åˆæ³• JSON
        return isinstance(text, str) and len(text.strip()) > 10

    # åªä¸¥æ ¼æ£€æŸ¥ inputï¼Œå¯¹ output å®½å®¹å¤„ç†
    valid_mask = data_df['input'].apply(is_valid_input_json) & data_df['output'].apply(is_valid_output)
    
    invalid_count = len(data_df) - valid_mask.sum()
    if invalid_count > 0:
        logging.warning(f"âš ï¸ Found {invalid_count} invalid records! Removing them...")
        data_df = data_df[valid_mask].reset_index(drop=True)
    
    logging.info(f"Cleaned dataset size: {len(data_df)}") # è¿™é‡Œåº”è¯¥æ¥è¿‘ 1589 æ‰å¯¹
    # 2. é¢„å¤„ç†æ•°æ®
    logging.info("Preprocessing judge dataset...")
    
    # [å…³é”®] æ ¹æ®æ•°æ®é›†è·å–å¯¹åº”çš„æ•…éšœç±»åˆ«åˆ—è¡¨
    current_fault_categories = DATASET_CONFIGS.get(args.dataset, DATASET_CONFIGS['cwru'])
    logging.info(f"Using fault categories for [{args.dataset}]: {current_fault_categories}")

    # [ä¿®æ­£ç‰ˆ] extract_winner_from_output
    def extract_winner_from_output(output_str):
        if not isinstance(output_str, str): return ""
        try:
            # æš´åŠ›æ­£åˆ™
            match = re.search(r'"winner"\s*:\s*"(.*?)"', output_str)
            if match:
                w = match.group(1).strip()
                # å¦‚æœæ—§æ•°æ®é‡Œæ··è¿›äº† Neitherï¼Œå¼ºåˆ¶è½¬ä¸º Model A
                if "Neither" in w or "bad" in w: return "Model A"
                return w
                
            # å…³é”®è¯å…œåº•
            if "Model A" in output_str: return "Model A"
            if "Model B" in output_str: return "Model B"
            if "Both" in output_str: return "Both are equally good"
            
            return ""
        except:
            return ""

    def extract_ground_truth_fault_type(input_str):
        try:
            input_dict = json.loads(input_str)
            return input_dict.get('ground_truth', '')
        except Exception:
            return ""

    def extract_model_diagnoses(input_str):
        try:
            input_dict = json.loads(input_str)
            model_a_response = input_dict.get('model_a_response', '')
            model_b_response = input_dict.get('model_b_response', '')
            
            # [æ ¸å¿ƒä¿®å¤] ä¸¥æ ¼æå–é€»è¾‘
            def extract_diagnosis_from_text(text):
                if not isinstance(text, str): return "Unknown"
                
                # 1. é»„é‡‘æ ‡å‡†ï¼šåªçœ‹ "Final Confirmed Diagnosis:" ä¹‹åçš„å†…å®¹
                if "Final Confirmed Diagnosis:" in text:
                    # å–åˆ†å‰²åçš„æœ€åä¸€éƒ¨åˆ†ï¼ˆé˜²æ­¢å‰é¢æœ‰å¼•ç”¨ï¼‰
                    # strip() å»æ‰å¯èƒ½å­˜åœ¨çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
                    target_area = text.split("Final Confirmed Diagnosis:")[-1].strip()
                    
                    # åœ¨è¿™ä¸ªæçŸ­çš„åŒºåŸŸå†…åŒ¹é…ï¼Œå‡†ç¡®ç‡ 100%
                    # æŒ‰ç…§é•¿åº¦å€’åºåŒ¹é… (é˜²æ­¢ Inner Race Fault è¢« Inner åŒ¹é…)
                    for fault in sorted(current_fault_categories, key=len, reverse=True):
                        # ä½¿ç”¨ lower() å¿½ç•¥å¤§å°å†™å·®å¼‚
                        if fault.lower() in target_area.lower():
                            return fault
                
                # 2. é“¶æ ‡å‡†ï¼šå¦‚æœç”Ÿæˆçš„æ ¼å¼ä¹±äº†ï¼Œåªæœç´¢æ–‡æœ¬çš„æœ€å 200 ä¸ªå­—ç¬¦
                # å› ä¸ºç»“è®ºé€šå¸¸åœ¨æœ€åã€‚è¿™æ ·èƒ½é¿å¼€å¼€å¤´ "Compared to Normal..." çš„å¹²æ‰°
                tail_text = text[-200:]
                for fault in sorted(current_fault_categories, key=len, reverse=True):
                    if fault.lower() in tail_text.lower():
                        return fault
                        
                return "Unknown"
            
            return extract_diagnosis_from_text(model_a_response), extract_diagnosis_from_text(model_b_response)
            
        except Exception as e:
            logging.warning(f"æå–è¯Šæ–­å¤±è´¥: {e}")
            return "Unknown", "Unknown"

    # åº”ç”¨æå–å‡½æ•°
    data_df['true_winner'] = data_df['output'].apply(extract_winner_from_output)
    data_df['ground_truth'] = data_df['input'].apply(extract_ground_truth_fault_type)
    
    diagnoses = data_df['input'].apply(extract_model_diagnoses).apply(pd.Series)
    data_df['model_a_diagnosis'] = diagnoses[0]
    data_df['model_b_diagnosis'] = diagnoses[1]
    
    data_df['true_winner_str'] = data_df['true_winner']
    data_df['true_fault_type'] = data_df['ground_truth']
    
    # æ ‡ç­¾æ˜ å°„
    winner_categories = ['Model A', 'Model B', 'Both are equally good']
    winner_to_label = {winner: idx for idx, winner in enumerate(winner_categories)}
    data_df['true_winner_label'] = data_df['true_winner'].map(winner_to_label).fillna(0)
    
    fault_to_label = {fault: idx for idx, fault in enumerate(current_fault_categories)}
    data_df['true_fault_label'] = data_df['true_fault_type'].map(fault_to_label).fillna(0)
    
    # 3. æ‹†åˆ†æ•°æ®é›†
    logging.info("Splitting dataset with stratification...")
    train_val_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df['true_winner_label'])
    train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42, stratify=train_val_df['true_winner_label'])
    
    train_df.attrs['name'] = 'train'
    val_df.attrs['name'] = 'val' 
    test_df.attrs['name'] = 'test'
    
    logging.info(f"Dataset sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")
     # [æ ¸å¿ƒä¿®æ”¹] è®­ç»ƒé›†æ¸…æ´—ç­–ç•¥ï¼šå‰”é™¤ "Both are equally good"
    # ==============================================================================
    if args.exclude_consensus:
        logging.info("\nğŸ›‘ [Strategy] Enabling 'Conflict-Only' Training...")
        logging.info("   Removing 'Both are equally good' samples from Training Set.")
        
        # æ‰¾åˆ° "Both are equally good" å¯¹åº”çš„ Label ID
        # winner_categories = ['Model A', 'Model B', 'Both are equally good']
        # é€šå¸¸ç´¢å¼•æ˜¯ 2ï¼Œä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬æŸ¥å­—å…¸
        if 'Both are equally good' in winner_to_label:
            both_label_id = winner_to_label['Both are equally good']
            
            # 1. è¿‡æ»¤è®­ç»ƒé›† (å¿…é¡»è¿‡æ»¤)
            initial_train_len = len(train_df)
            train_df = train_df[train_df['true_winner_label'] != both_label_id].copy()
            logging.info(f"   ğŸ“‰ Train Set reduced: {initial_train_len} -> {len(train_df)} (Dropped Consensus)")
            
            # 2. è¿‡æ»¤éªŒè¯é›† (å¯é€‰ï¼Œå»ºè®®ä¹Ÿè¿‡æ»¤ï¼Œä»¥ä¾¿è§‚å¯Ÿæ¨¡å‹åœ¨å›°éš¾æ ·æœ¬ä¸Šçš„Losså˜åŒ–)
            # å¦‚æœä¸è¿‡æ»¤éªŒè¯é›†ï¼ŒLoss å¯èƒ½ä¼šå¾ˆä½ï¼ˆå› ä¸ºç®€å•é¢˜å¤šï¼‰ï¼Œæ©ç›–äº†æ¨¡å‹åœ¨éš¾é¢˜ä¸Šçš„ç³Ÿç³•è¡¨ç°
            initial_val_len = len(val_df)
            val_df = val_df[val_df['true_winner_label'] != both_label_id].copy()
            logging.info(f"   ğŸ“‰ Val Set reduced:   {initial_val_len} -> {len(val_df)} (Dropped Consensus)")
            
            # 3. æµ‹è¯•é›† (Test Set) -> ç»å¯¹ä¸åŠ¨ï¼ä¿æŒå…¨é‡ï¼
            logging.info(f"   ğŸ›¡ï¸ Test Set remains FULL size ({len(test_df)}) to reflect real-world distribution.")
        else:
            logging.warning("âš ï¸ 'Both are equally good' not found in categories. Skipping filter.")
    # 4. è°ƒç”¨è®­ç»ƒ
    train_and_evaluate(args, train_df, val_df, test_df, winner_categories)

if __name__ == "__main__":
    log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers(): root_logger.handlers.clear()
    
    # æ—¥å¿—æ–‡ä»¶ä¹ŸåŠ ä¸Šæ—¶é—´æˆ³æˆ– dataset åæ¯”è¾ƒå¥½ï¼Œè¿™é‡Œå…ˆä¿æŒåŸæ ·
    log_file = 'finetune_judge.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)
    
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        logging.error("An unhandled exception occurred:", exc_info=True)
        sys.exit(1)